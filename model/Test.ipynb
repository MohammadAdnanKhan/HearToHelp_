{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EmotionRecognizer:\n",
        "    def __init__(self, model_path='model.h5'):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.required_time_steps = 352\n",
        "        self.emotion_map = {\n",
        "            0: 'neutral',\n",
        "            1: 'happy',\n",
        "            2: 'sad',\n",
        "            3: 'angry',\n",
        "            4: 'fear',\n",
        "            5: 'disgust'\n",
        "        }\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained model.\"\"\"\n",
        "        try:\n",
        "            self.model = load_model(self.model_path)\n",
        "            print(\"Model loaded successfully\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def extract_features(self, audio_path):\n",
        "        \"\"\"Extract and normalize audio features matching the original 15 features.\"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            y, sr = librosa.load(audio_path, sr=22050, duration=3)\n",
        "\n",
        "            # Set consistent parameters\n",
        "            frame_length = 2048\n",
        "            hop_length = 512\n",
        "\n",
        "            # Extract only the original features\n",
        "            zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)\n",
        "            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=hop_length)\n",
        "\n",
        "            # Print shapes for debugging\n",
        "            print(\"\\nFeature shapes before processing:\")\n",
        "            print(f\"ZCR shape: {zcr.shape}\")\n",
        "            print(f\"RMS shape: {rms.shape}\")\n",
        "            print(f\"MFCCs shape: {mfccs.shape}\")\n",
        "\n",
        "            # Ensure all features have the same time steps\n",
        "            min_time_steps = min(zcr.shape[1], rms.shape[1], mfccs.shape[1])\n",
        "\n",
        "            # Trim all features to minimum length\n",
        "            zcr = zcr[:, :min_time_steps]\n",
        "            rms = rms[:, :min_time_steps]\n",
        "            mfccs = mfccs[:, :min_time_steps]\n",
        "\n",
        "            # Stack features (15 features total: 1 ZCR + 1 RMS + 13 MFCCs)\n",
        "            features = np.vstack([zcr, rms, mfccs])\n",
        "\n",
        "            # Handle time steps\n",
        "            if features.shape[1] < self.required_time_steps:\n",
        "                # Use repetition instead of zero padding\n",
        "                repeat_times = self.required_time_steps // features.shape[1] + 1\n",
        "                features = np.tile(features, (1, repeat_times))[:, :self.required_time_steps]\n",
        "            elif features.shape[1] > self.required_time_steps:\n",
        "                # Take center portion\n",
        "                start_idx = (features.shape[1] - self.required_time_steps) // 2\n",
        "                features = features[:, start_idx:start_idx + self.required_time_steps]\n",
        "\n",
        "            # Transpose to (time_steps, features)\n",
        "            features = features.T\n",
        "\n",
        "            # Initialize scaler if needed\n",
        "            if self.scaler is None:\n",
        "                self.scaler = StandardScaler()\n",
        "                features_normalized = self.scaler.fit_transform(features)\n",
        "            else:\n",
        "                features_normalized = self.scaler.transform(features)\n",
        "\n",
        "            # Expand dimensions for model input\n",
        "            features_final = np.expand_dims(features_normalized, axis=0)\n",
        "\n",
        "            print(f\"\\nFinal feature shape: {features_final.shape}\")\n",
        "            return features_final\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in feature extraction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict_emotion(self, audio_path):\n",
        "        \"\"\"Predict emotion with detailed probability output.\"\"\"\n",
        "        try:\n",
        "            # Load model if not already loaded\n",
        "            if self.model is None:\n",
        "                if not self.load_model():\n",
        "                    return None, None, None\n",
        "\n",
        "            # Extract features\n",
        "            X_input = self.extract_features(audio_path)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = self.model.predict(X_input, verbose=0)\n",
        "\n",
        "            # Get probabilities for all emotions\n",
        "            emotion_probabilities = {\n",
        "                self.emotion_map[i]: float(prob)\n",
        "                for i, prob in enumerate(predictions[0])\n",
        "            }\n",
        "\n",
        "            # Get predicted emotion and confidence\n",
        "            predicted_class = np.argmax(predictions[0])\n",
        "            predicted_emotion = self.emotion_map[predicted_class]\n",
        "            confidence = predictions[0][predicted_class]\n",
        "\n",
        "            return predicted_emotion, confidence, emotion_probabilities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def process_directory(self, directory_path):\n",
        "        \"\"\"Process all audio files in a directory.\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for file in os.listdir(directory_path):\n",
        "            if file.endswith(('.wav', '.mp3')):\n",
        "                print(f\"\\nProcessing: {file}\")\n",
        "                audio_path = os.path.join(directory_path, file)\n",
        "                emotion, confidence, probs = self.predict_emotion(audio_path)\n",
        "\n",
        "                if emotion:\n",
        "                    results.append({\n",
        "                        'file': file,\n",
        "                        'emotion': emotion,\n",
        "                        'confidence': confidence,\n",
        "                        'probabilities': probs\n",
        "                    })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "7slCr3Oq7Cwe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvA24Pte6jje",
        "outputId": "d905e28d-444d-4db3-924f-114f6cbfb7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n",
            "\n",
            "Feature shapes before processing:\n",
            "ZCR shape: (1, 130)\n",
            "RMS shape: (1, 130)\n",
            "MFCCs shape: (13, 130)\n",
            "\n",
            "Final feature shape: (1, 352, 15)\n",
            "\n",
            "Prediction Results:\n",
            "Predicted Emotion: sad\n",
            "Confidence: 73.09%\n",
            "\n",
            "Probabilities for all emotions:\n",
            "neutral: 11.03%\n",
            "happy: 6.33%\n",
            "sad: 73.09%\n",
            "angry: 1.32%\n",
            "fear: 3.76%\n",
            "disgust: 4.48%\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Initialize the emotion recognizer\n",
        "    recognizer = EmotionRecognizer('model.h5')\n",
        "\n",
        "    # Test single file\n",
        "    audio_path = \"/content/call_142_0.wav\"\n",
        "\n",
        "    try:\n",
        "        emotion, confidence, probabilities = recognizer.predict_emotion(audio_path)\n",
        "\n",
        "        if emotion:\n",
        "            print(\"\\nPrediction Results:\")\n",
        "            print(f\"Predicted Emotion: {emotion}\")\n",
        "            print(f\"Confidence: {confidence:.2%}\")\n",
        "            print(\"\\nProbabilities for all emotions:\")\n",
        "            for emotion_name, prob in probabilities.items():\n",
        "                print(f\"{emotion_name}: {prob:.2%}\")\n",
        "        else:\n",
        "            print(\"Failed to predict emotion.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvdsb3EI68U5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}